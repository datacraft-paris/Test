{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf218dc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/adrianahne/PhD/causality/Causal-associations-diabetes-twitter/data/cause_effect_sentences_with_IO_tags.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bcfa5d39b619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdataPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/adrianahne/PhD/causality/Causal-associations-diabetes-twitter/data/cause_effect_sentences_with_IO_tags.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"tokenized\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bio_tags\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/adrianahne/PhD/causality/Causal-associations-diabetes-twitter/data/cause_effect_sentences_with_IO_tags.csv'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "########################### MODEL PARAMETERS ############################\n",
    "train_to_test_ratio = 0.9 # 10% test and 90% train\n",
    "\n",
    "bert_model = \"vinai/bertweet-base\" # \"bert-large-uncased\"; \"roberta-large\"\n",
    "\n",
    "\n",
    "##### DATA TO LOAD ######\n",
    "dataPath = \"/Users/adrianahne/PhD/causality/Causal-associations-diabetes-twitter/data/cause_effect_sentences_with_IO_tags.csv\"\n",
    "\n",
    "data = pd.read_csv(dataPath, sep=\";\", converters={\"tokenized\":literal_eval, \"bio_tags\":literal_eval})\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05cc1e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  False\n",
      "Selected cpu for this notebook\n"
     ]
    }
   ],
   "source": [
    "########################### Check if cuda available ############################\n",
    "print(\"Cuda available: \", torch.cuda.is_available())\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Selected {} for this notebook\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b25e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2118, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>bio_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USER Additionally the medicines are being char...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medicines are being charged at MRP</td>\n",
       "      <td>costing much higher</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[USER, Additionally, the, medicines, are, bein...</td>\n",
       "      <td>[O, O, O, I-C, I-C, I-C, I-C, I-C, I-C, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hear \" I hate being a diabetic \" .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>diabetic</td>\n",
       "      <td>hate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[I, hear, \", I, hate, being, a, diabetic, \", .]</td>\n",
       "      <td>[O, O, O, O, I-E, O, O, I-C, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i got lime for my glucose test , was n't that ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>glucose test</td>\n",
       "      <td>nauseous</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[i, got, lime, for, my, glucose, test, ,, was,...</td>\n",
       "      <td>[O, O, O, O, O, I-C, I-C, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sounds like Willow 's blood sugar level is rea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blood sugar level is real low</td>\n",
       "      <td>reduce her insulin shots</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Sounds, like, Willow, 's, blood, sugar, level...</td>\n",
       "      <td>[O, O, O, O, I-C, I-C, I-C, I-C, I-C, I-C, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER I 've always found it too sweet mustvsay ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dreaded diabetes</td>\n",
       "      <td>sauces are used sparingly</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[USER, I, 've, always, found, it, too, sweet, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, I-E, I-E,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence Intent  \\\n",
       "0  USER Additionally the medicines are being char...    NaN   \n",
       "1               I hear \" I hate being a diabetic \" .    NaN   \n",
       "2  i got lime for my glucose test , was n't that ...    NaN   \n",
       "3  Sounds like Willow 's blood sugar level is rea...    NaN   \n",
       "4  USER I 've always found it too sweet mustvsay ...    NaN   \n",
       "\n",
       "                                Cause                     Effect  \\\n",
       "0  medicines are being charged at MRP        costing much higher   \n",
       "1                            diabetic                       hate   \n",
       "2                        glucose test                   nauseous   \n",
       "3       blood sugar level is real low   reduce her insulin shots   \n",
       "4                    dreaded diabetes  sauces are used sparingly   \n",
       "\n",
       "   Causal association                                          tokenized  \\\n",
       "0                 1.0  [USER, Additionally, the, medicines, are, bein...   \n",
       "1                 1.0    [I, hear, \", I, hate, being, a, diabetic, \", .]   \n",
       "2                 1.0  [i, got, lime, for, my, glucose, test, ,, was,...   \n",
       "3                 1.0  [Sounds, like, Willow, 's, blood, sugar, level...   \n",
       "4                 1.0  [USER, I, 've, always, found, it, too, sweet, ...   \n",
       "\n",
       "                                            bio_tags  \n",
       "0  [O, O, O, I-C, I-C, I-C, I-C, I-C, I-C, O, O, ...  \n",
       "1                 [O, O, O, O, I-E, O, O, I-C, O, O]  \n",
       "2  [O, O, O, O, O, I-C, I-C, O, O, O, O, O, O, O,...  \n",
       "3  [O, O, O, O, I-C, I-C, I-C, I-C, I-C, I-C, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, I-E, I-E,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############ Choose only sentences with both cause and effect or only sentences with either cause or effect (or both) #######\n",
    "\n",
    "dataSentFiltered = data[(data[\"Cause\"].notnull()) & (data[\"Effect\"].notnull())]\n",
    "\n",
    "print(dataSentFiltered.shape)\n",
    "dataSentFiltered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80c19da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1906, 7)\n",
      "Test: (212, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>bio_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>Benefit to having a child when diabetic : you ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blood sugar is low</td>\n",
       "      <td>lollies</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Benefit, to, having, a, child, when, diabetic...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, I-E, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>@USER my father suffering from diabetes INSULI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>suffering</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[@USER, my, father, suffering, from, diabetes,...</td>\n",
       "      <td>[O, O, O, I-E, O, I-C, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>I did n't knew that diabetes can be one of sid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thyroid disorder</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[I, did, n't, knew, that, diabetes, can, be, o...</td>\n",
       "      <td>[O, O, O, O, O, I-E, O, O, O, O, O, O, O, I-C,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>My blood sugar has been high all evening and m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blood sugar has been high</td>\n",
       "      <td>body hurts</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[My, blood, sugar, has, been, high, all, eveni...</td>\n",
       "      <td>[O, I-C, I-C, I-C, I-C, I-C, O, O, O, O, O, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>@USER Cool , I spent the holiday talking about...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>struggling</td>\n",
       "      <td>#insulin4all</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[@USER, Cool, ,, I, spent, the, holiday, talki...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, I-E, O, O, O, I-C, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence Intent  \\\n",
       "1445  Benefit to having a child when diabetic : you ...    NaN   \n",
       "1476  @USER my father suffering from diabetes INSULI...    NaN   \n",
       "2130  I did n't knew that diabetes can be one of sid...    NaN   \n",
       "1320  My blood sugar has been high all evening and m...    NaN   \n",
       "2247  @USER Cool , I spent the holiday talking about...    NaN   \n",
       "\n",
       "                          Cause        Effect  Causal association  \\\n",
       "1445         blood sugar is low       lollies                 1.0   \n",
       "1476                   diabetes     suffering                 1.0   \n",
       "2130           Thyroid disorder      diabetes                 1.0   \n",
       "1320  blood sugar has been high    body hurts                 1.0   \n",
       "2247                 struggling  #insulin4all                 1.0   \n",
       "\n",
       "                                              tokenized  \\\n",
       "1445  [Benefit, to, having, a, child, when, diabetic...   \n",
       "1476  [@USER, my, father, suffering, from, diabetes,...   \n",
       "2130  [I, did, n't, knew, that, diabetes, can, be, o...   \n",
       "1320  [My, blood, sugar, has, been, high, all, eveni...   \n",
       "2247  [@USER, Cool, ,, I, spent, the, holiday, talki...   \n",
       "\n",
       "                                               bio_tags  \n",
       "1445  [O, O, O, O, O, O, O, O, O, O, O, O, O, I-E, O...  \n",
       "1476  [O, O, O, I-E, O, I-C, O, O, O, O, O, O, O, O,...  \n",
       "2130  [O, O, O, O, O, I-E, O, O, O, O, O, O, O, I-C,...  \n",
       "1320  [O, I-C, I-C, I-C, I-C, I-C, O, O, O, O, O, I-...  \n",
       "2247  [O, O, O, O, O, O, O, O, O, I-E, O, O, O, I-C, O]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingDataSample = dataSentFiltered#.sample(n=100)   # Only for testing\n",
    "train = trainingDataSample.sample(frac=train_to_test_ratio, random_state=0)\n",
    "test = trainingDataSample.drop(train.index)\n",
    "print(\"Train:\", train.shape)\n",
    "print(\"Test:\", test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca914a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(bert_model, padding = \"max_length\", truncation = True, max_length = 60, return_offsets_mapping=True )\n",
    "model = AutoModel.from_pretrained(bert_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e5f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(sentence, sentence_tokenised):\n",
    "    ids = tokenizer.encode(sentence) \n",
    "    ids_tensor = torch.tensor(ids).unsqueeze(0) # Batch size: 1\n",
    "    word_vectors = model(ids_tensor)[0].squeeze()\n",
    "    \n",
    "    # get single vector for each word. (Sub words are averaged)\n",
    "    word_embeddings_all = []\n",
    "    for word in sentence_tokenised:\n",
    "        word_encoded = tokenizer.encode(word)\n",
    "        word_encoded.remove(tokenizer.cls_token_id) # we don't want <CLS> token embedding\n",
    "        word_encoded.remove(tokenizer.sep_token_id) # we don't want <SEP> token embedding\n",
    "        #print(\"word_encoded:\", word_encoded)\n",
    "        \n",
    "        word_indices = [ids.index(encoded_id) for encoded_id in word_encoded ] \n",
    "        #print(\"word_indices:\", word_indices)\n",
    "        \n",
    "        # average all sub_word vectors of word\n",
    "        word_vector = torch.zeros((768))\n",
    "        for sub_token_id in word_indices:\n",
    "            word_vector += word_vectors[sub_token_id]\n",
    "        word_vector /= len(word_indices)\n",
    "        \n",
    "        word_embeddings_all.append(word_vector)\n",
    "        \n",
    "    return word_embeddings_all\n",
    "\n",
    "def word2features(word, i, wordembedding):\n",
    "\n",
    "    features = {\n",
    "#        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "#        'word[-3:]': word[-3:],\n",
    "#        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "#        'postag': postag,\n",
    "#        'postag[:2]': postag[:2],\n",
    "        'wordlength': len(word),\n",
    "        'wordinitialcap': word[0].isupper(),\n",
    "        'wordmixedcap': len([x for x in word[1:] if x.isupper()])>0,\n",
    "        'wordallcap': len([x for x in word if x.isupper()])==len(word),\n",
    "        'distfromsentbegin': i\n",
    "    }\n",
    "\n",
    "    # here you add 768 features (one for each vector component)\n",
    "    for iv,value in enumerate(wordembedding):\n",
    "        features['v{}'.format(iv)]=value\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def sent2features(sentence, tokenized):\n",
    "    word_vectors = get_word_embeddings(sentence, tokenized)\n",
    "    return [word2features(tokenized[i], i, word_vectors[i]) for i in range(len(tokenized))]\n",
    "\n",
    "\n",
    "X_train = [sent2features(sentence, tokenized) for sentence, tokenized in zip(train.sentence.values.tolist(), train.tokenized.values.tolist())]\n",
    "y_train = [tags for tags in train.bio_tags]\n",
    "print(\"X_train:\", len(X_train), len(X_train[0]))\n",
    "print(\"y_train:\", len(y_train), len(y_train[0]))\n",
    "\n",
    "X_test = [sent2features(sentence, tokenized) for sentence, tokenized in zip(test.sentence.values.tolist(), test.tokenized.values.tolist())]\n",
    "y_test = [tags for tags in test.bio_tags]\n",
    "\n",
    "print(\"X_test:\", len(X_test), len(X_test[0]))\n",
    "print(\"y_test:\", len(y_test), len(y_test[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11438999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='vinai/bertweet-base', vocab_size=64000, model_max_len=128, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ea514",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=False\n",
    ")\n",
    "crf.fit(X_train, y_train)   ### Error message when try to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a906a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4846045",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_tag = [ID for ID in np.concatenate(y_test)]#\n",
    "test_predict_tag = [ID for ID in np.concatenate(predictions)]\n",
    "print(classification_report(test_true_tag, test_predict_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952ea7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens, true_labels, predicts in zip(test.tokenized, test.bio_tags, predictions):\n",
    "    print(\"\\n\")\n",
    "    for token, true_label, predic in zip(tokens, true_labels, predicts):\n",
    "        print(token, \"true:\", true_label, \"predic:\", predic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
