{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02666b54",
   "metadata": {},
   "source": [
    "## FastText embedding features in single CRF layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3cb3810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji import demojize\n",
    "import re\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "# https://huggingface.co/vinai/bertweet-base\n",
    "def normalizeToken(token):\n",
    "    lowercased_token = token.lower()\n",
    "    if token.startswith(\"@\"):\n",
    "        return \"@USER\"\n",
    "    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
    "        return \"HTTPURL\"\n",
    "    elif len(token) == 1:\n",
    "        return demojize(token)\n",
    "    else:\n",
    "        if token == \"’\":\n",
    "            return \"'\"\n",
    "        elif token == \"…\":\n",
    "            return \"...\"\n",
    "        else:\n",
    "            return token\n",
    "\n",
    "        \n",
    "def normalizeTweet(tweet):\n",
    "\n",
    "    tokens = tweet_tokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n",
    "    normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \", \" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n",
    "    normTweet = normTweet.replace(\" p . m .\", \"  p.m.\") .replace(\" p . m \", \" p.m \").replace(\" a . m .\", \" a.m.\").replace(\" a . m \", \" a.m \")\n",
    "\n",
    "    normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n",
    "    normTweet = re.sub(r\"([0-9]{1,3}) / ([0-9]{2,4})\", r\"\\1/\\2\", normTweet)\n",
    "    normTweet = re.sub(r\"([0-9]{1,3})- ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n",
    "    \n",
    "    return \" \".join(normTweet.split())\n",
    "\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\" Split tweet into sentences \"\"\"\n",
    "    \n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\"..\", \"<POINTPOINT>\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"<POINTPOINT>\", \"..\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s  for s in sentences if s != \"\"]\n",
    "    return sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009eb9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"data/cause_effect_sentences_with_IO_tags.csv\"\n",
    "data = pd.read_csv(dataPath, sep=\";\", converters={\"tokenized\":literal_eval, \"bio_tags\":literal_eval})\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4dea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in data[0:20].iterrows():\n",
    "    print(\"\\n\", row[\"sentence\"])\n",
    "    print(row[\"tokenized\"])\n",
    "    print(row[\"bio_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbba7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Take only sentences with both cause and effect\n",
    "\n",
    "data_sentences = data[(data[\"Cause\"].notnull()) & (data[\"Effect\"].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5bd127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Load FastText embeddings (trained on diabetes tweets) #######\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "we_path = \"models/FastText_embeddings/ft_wordembeddings_dim300_minCount5_URL-User-toConstant_iter10_20190703\"\n",
    "wordEmbeddings = FastText.load(we_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e72ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_test_ratio = 0.9\n",
    "train = data_sentences.sample(frac=train_to_test_ratio, random_state=0)\n",
    "test = data_sentences.drop(train.index)\n",
    "print(\"Train:\", train.shape)\n",
    "print(\"Test:\", test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fba7105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_5874/3658095869.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  vector=wordEmbeddings[word]\n"
     ]
    }
   ],
   "source": [
    "########## Create features for conditional random field (CRF) #######\n",
    "\n",
    "def get_features(word):\n",
    "    word=word.lower()\n",
    "    try:\n",
    "         vector=wordEmbeddings[word]\n",
    "    except:\n",
    "        # if the word is not in vocabulary, returns zeros array\n",
    "        vector=np.zeros(300,)\n",
    "\n",
    "    return vector   \n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i]#[0]\n",
    "    wordembedding=get_features(word)   ## word embedding vector \n",
    "\n",
    "    # features to return\n",
    "    # TODO: add / remove features\n",
    "    features = {\n",
    "#        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "#        'word[-3:]': word[-3:],\n",
    "#        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "#        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "#        'postag': postag,\n",
    "#        'postag[:2]': postag[:2],\n",
    "        'wordlength': len(word),\n",
    "        'wordinitialcap': word[0].isupper(),\n",
    "        'wordmixedcap': len([x for x in word[1:] if x.isupper()])>0,\n",
    "        'wordallcap': len([x for x in word if x.isupper()])==len(word),\n",
    "        'distfromsentbegin': i\n",
    "    }\n",
    "\n",
    "    # here you add 300 features (one for each vector component)\n",
    "    for iv,value in enumerate(wordembedding):\n",
    "        features['v{}'.format(iv)]=value\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    \"\"\" Get feature vector for each sentence \"\"\"\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "\n",
    "X_train = [sent2features(sentence) for sentence in train.tokenized.values.tolist()]\n",
    "y_train = [tags for tags in train.bio_tags]\n",
    "\n",
    "\n",
    "X_test = [sent2features(sentence) for sentence in test.tokenized.values.tolist()]\n",
    "y_test = [tags for tags in test.bio_tags]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7defc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a491087",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## TRAIN model #########\n",
    "import sklearn_crfsuite\n",
    "\n",
    "%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=False\n",
    ")\n",
    "crf.fit(X_train, y_train)   ### Error message when try to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e3700c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Test set predictions ########\n",
    "predictions = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a22bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Performance measures ############\n",
    "\n",
    "test_true_tag = [ID for ID in np.concatenate(y_test)]#\n",
    "test_predict_tag = [ID for ID in np.concatenate(predictions)]\n",
    "print(classification_report(test_true_tag, test_predict_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01132a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "317c17ac",
   "metadata": {},
   "source": [
    "TODO: Change the features in the function word2features and play with the model parameters to beat this baseline:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         I-C       0.59      0.57      0.58       495\n",
    "         I-E       0.45      0.38      0.41       487\n",
    "           O       0.92      0.94      0.93      4159\n",
    "\n",
    "    accuracy                           0.85      5141\n",
    "    macro avg      0.65      0.63      0.64      5141\n",
    "    weighted avg   0.84      0.85      0.84      5141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b181a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Save your best model ############\n",
    "#joblib.dump(\".....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02457a8",
   "metadata": {},
   "source": [
    "## Apply on diabetes tweets\n",
    "\n",
    "Consider normalizing and splitting into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5545b561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13310</th>\n",
       "      <td>@USER w / r to this particular point Jeremy as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42148</th>\n",
       "      <td>This is the same parent who , when I drank loa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48035</th>\n",
       "      <td>When ur blood sugar :syringe: is 46 and u feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45706</th>\n",
       "      <td>@USER You really need to go . More people die ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>That 's right ! No one said rice does n't fill...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "13310  @USER w / r to this particular point Jeremy as...\n",
       "42148  This is the same parent who , when I drank loa...\n",
       "48035  When ur blood sugar :syringe: is 46 and u feel...\n",
       "45706  @USER You really need to go . More people die ...\n",
       "949    That 's right ! No one said rice does n't fill..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## Load diabetes tweets ##############\n",
    "\n",
    "\n",
    "diabetes_tweets = pd.read_csv(\"data/diabetes_tweets_normalized.csv\", sep=\";\")\n",
    "diabetes_tweets = diabetes_tweets.sample(n=1000, random_state=55)\n",
    "print(diabetes_tweets.shape)\n",
    "diabetes_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c89550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets to sentences: 2565\n",
      "['@USER w / r to this particular point Jeremy as a T1 diabetic ( 47yrs ) I carry ID in my wallet to identify mys ...'\n",
      " 'HTTPURL'\n",
      " 'This is the same parent who , when I drank loads of tea BECAUSE TEA IS PENG , assumed I was pre diabetic and “ thirsty ...'\n",
      " 'HTTPURL'\n",
      " 'When ur blood sugar : syringe : is 46 and u feel like eating the entire pantry : woozy_face : #diabeticproblems #type1 #diabetes'\n",
      " '@USER You really need to go .'\n",
      " 'More people die of diabetes every year .'\n",
      " 'Less would have died had you not put cov ...' 'HTTPURL'\n",
      " \"That ' s right !\"]\n"
     ]
    }
   ],
   "source": [
    "####### SPLIT TWEETS INTO SENTENCES ######################\n",
    "\n",
    "diabetes_sentences = diabetes_tweets[\"text\"].map(lambda text: split_into_sentences(normalizeTweet(text)))\n",
    "diabetes_sentences = diabetes_sentences.explode()\n",
    "print(\"tweets to sentences:\", diabetes_sentences.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7534470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N sentences with > 5 words & no question: (1464,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13310</th>\n",
       "      <td>@USER w / r to this particular point Jeremy as...</td>\n",
       "      <td>[@USER, w, /, r, to, this, particular, point, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42148</th>\n",
       "      <td>This is the same parent who , when I drank loa...</td>\n",
       "      <td>[This, is, the, same, parent, who, ,, when, I,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48035</th>\n",
       "      <td>When ur blood sugar : syringe : is 46 and u fe...</td>\n",
       "      <td>[When, ur, blood, sugar, :, syringe, :, is, 46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45706</th>\n",
       "      <td>@USER You really need to go .</td>\n",
       "      <td>[@USER, You, really, need, to, go, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45706</th>\n",
       "      <td>More people die of diabetes every year .</td>\n",
       "      <td>[More, people, die, of, diabetes, every, year, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  \\\n",
       "13310  @USER w / r to this particular point Jeremy as...   \n",
       "42148  This is the same parent who , when I drank loa...   \n",
       "48035  When ur blood sugar : syringe : is 46 and u fe...   \n",
       "45706                      @USER You really need to go .   \n",
       "45706           More people die of diabetes every year .   \n",
       "\n",
       "                                               tokenized  \n",
       "13310  [@USER, w, /, r, to, this, particular, point, ...  \n",
       "42148  [This, is, the, same, parent, who, ,, when, I,...  \n",
       "48035  [When, ur, blood, sugar, :, syringe, :, is, 46...  \n",
       "45706              [@USER, You, really, need, to, go, .]  \n",
       "45706  [More, people, die, of, diabetes, every, year, .]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### Exclude questions and sentences with less than 5 words #################\n",
    "\n",
    "diabetes_sentences_filtered = diabetes_sentences[diabetes_sentences.str.split(\" \").str.len() > 5] # keep only sentence with more than 3 tokens\n",
    "diabetes_sentences_filtered = diabetes_sentences_filtered[~diabetes_sentences_filtered.str.endswith(\"?\")]\n",
    "print(\"N sentences with > 5 words & no question:\", diabetes_sentences_filtered.shape)\n",
    "\n",
    "diabetes_sentences_filtered_df = diabetes_sentences_filtered.to_frame(\"sentences\")\n",
    "diabetes_sentences_filtered_df[\"tokenized\"] = diabetes_sentences_filtered_df[\"sentences\"].map(lambda tweet: normalizeTweet(tweet).split(\" \"))\n",
    "diabetes_sentences_filtered_df.head()\n",
    "#diabetes_text = diabetes_sentences_filtered.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5de66da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_5874/3658095869.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  vector=wordEmbeddings[word]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 1906 31\n"
     ]
    }
   ],
   "source": [
    "diabetes_features = [sent2features(sentence) for sentence in diabetes_sentences_filtered_df.tokenized.values.tolist()]\n",
    "print(\"X_train:\", len(X_train), len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6db2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_predictions = crf.predict(diabetes_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce33ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens, predicts in zip(diabetes_sentences_filtered_df.tokenized, diabetes_predictions):\n",
    "    print(\"\\n\")\n",
    "    for token, predic in zip(tokens, predicts):\n",
    "        print(token, \"true:\", true_label, \"predic:\", predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955651ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5948bd5f",
   "metadata": {},
   "source": [
    "## Apply on personal cancer tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b3e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Change Path to Causal cancer sentences\n",
    "\n",
    "######### LOAD cancer tweets #################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Alternatively, take the smaller file: data/causal_cancer_sentences_personal_subsample.csv\n",
    "tweets_cancer = pd.read_csv(\"data/causal_cancer_sentences_personal.csv\", sep=\";\")\n",
    "print(tweets_cancer.shape)\n",
    "tweets_cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe45b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c61714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7117c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a9e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f85db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa2e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4402ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Save file ##################\n",
    "\n",
    "# Save DataFrame with text and predicted cause and effect\n",
    "\n",
    "cancer_cause_effect.to_csv(\"data/cancer_cause_effect_FastText.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a1b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55804594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
